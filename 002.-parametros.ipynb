{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clave de OpenAI se ha cargado correctamente.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar las variables del archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener la clave de OpenAI\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Verifica que la clave se ha cargado correctamente\n",
    "if openai_api_key:\n",
    "    print(\"La clave de OpenAI se ha cargado correctamente.\")\n",
    "else:\n",
    "    print(\"Error al cargar la clave de OpenAI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client=OpenAI(api_key=openai_api_key)\n",
    "MODEL='gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el contexto de modelos de lenguaje como GPT-3.5-turbo, **temperature** y **seed** son dos parámetros importantes que influyen en el comportamiento y la salida del modelo.\n",
    "\n",
    "### **Temperature**:\n",
    "La **temperature** es un parámetro que controla el nivel de creatividad y aleatoriedad en las respuestas generadas por el modelo. Se utiliza para ajustar cuán predecible o impredecible serán las salidas del modelo. Su rango generalmente es de 0 a 2:\n",
    "\n",
    "- **Temperature baja (0 a 0.5)**: El modelo será más conservador y tiende a generar respuestas más deterministas y seguras. Por ejemplo, con una temperatura de 0, el modelo siempre dará la misma respuesta ante la misma entrada.\n",
    "  \n",
    "- **Temperature alta (mayor a 1)**: Genera más aleatoriedad en las respuestas. Esto significa que el modelo puede ser más creativo y menos predecible, lo que puede llevar a respuestas más variadas, pero a veces menos coherentes o más \"alucinadas\".\n",
    "  \n",
    "En tu ejemplo:\n",
    "\n",
    "```python\n",
    "temperature=0.1\n",
    "```\n",
    "\n",
    "Con un valor de **0.1**, el modelo estará ajustado para generar respuestas bastante seguras y coherentes, con muy poca aleatoriedad.\n",
    "\n",
    "### **Seed**:\n",
    "El parámetro **seed** es una semilla que controla el factor de **determinismo** en la generación de texto. Específicamente, se usa para asegurar que se obtenga la misma salida cada vez que se utiliza el mismo conjunto de entradas y parámetros.\n",
    "\n",
    "Cuando fijas una semilla con un valor numérico:\n",
    "\n",
    "```python\n",
    "seed=1234\n",
    "```\n",
    "\n",
    "Esto asegura que, siempre que ejecutes el mismo código con los mismos datos de entrada y la misma semilla, el modelo generará la misma respuesta. Es útil para la **reproducibilidad** de resultados, ya que si alguien más ejecuta el mismo código con la misma semilla, obtendrá la misma salida.\n",
    "\n",
    "Resumiendo:\n",
    "- **Temperature** ajusta el grado de creatividad y aleatoriedad en la generación de texto.\n",
    "- **Seed** asegura que las salidas del modelo sean reproducibles, generando los mismos resultados para las mismas entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {'role':'system','content':'Eres un buen asistente de ayuda'},\n",
    "        {'role':'user','content':'Dame consejos para monetizar mi canal de youtube'},\n",
    "    ],\n",
    "    # temperature entre 0 y 2, por defecto 1 cuano más alto mayor la alucinación\n",
    "    temperature=1,\n",
    "    seed=1234\n",
    "    \n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {'role':'system','content':'Eres un buen asistente de ayuda'},\n",
    "        {'role':'user','content':'Dame consejos para monetizar mi canal de youtube'},\n",
    "    ],\n",
    "    # temperature entre 0 y 2, por defecto 1 cuano más alto mayor la alucinación\n",
    "    temperature=0.1,\n",
    "    seed=1234\n",
    "    \n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"MODEL {MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.system_fingerprint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top\n",
    "El parámetro **`top_p`** en la generación de texto con modelos de lenguaje como GPT-3.5 (usado en tu ejemplo) está relacionado con un método de muestreo llamado **nucleus sampling** (muestreo por núcleo). Este método ajusta el rango de palabras que el modelo puede considerar al generar texto en función de sus probabilidades acumuladas.\n",
    "\n",
    "Aquí te explico el concepto más detalladamente:\n",
    "\n",
    "- **`top_p`** define el porcentaje de probabilidad acumulada a partir del cual se seleccionarán las palabras para cada paso de generación. Por ejemplo:\n",
    "  \n",
    "  - Si **`top_p=1`**, significa que el modelo considerará **todas** las palabras posibles para la predicción, lo cual sería similar a no aplicar restricciones de probabilidad (comportamiento predeterminado).\n",
    "  \n",
    "  - Si **`top_p=0.1`**, el modelo solo considerará aquellas palabras cuyas probabilidades sumen el **10%** de la probabilidad acumulada más alta. Esto hace que el modelo sea más \"conservador\", limitando su salida a un conjunto más reducido de palabras con alta probabilidad.\n",
    "\n",
    "- **¿Qué implica esto en la práctica?**\n",
    "  \n",
    "  - Un valor bajo de `top_p` (como 0.1) puede hacer que las respuestas sean más predecibles y se concentren en las palabras más probables, descartando las menos probables. Esto puede reducir la creatividad o variación en las respuestas, pero también reduce la posibilidad de generar respuestas irrelevantes o incoherentes.\n",
    "  \n",
    "  - Un valor alto de `top_p` (como 0.9 o 1) permite que el modelo sea más flexible, eligiendo palabras menos probables, lo que puede aumentar la creatividad, pero también el riesgo de generar respuestas menos precisas o \"alucinaciones\".\n",
    "\n",
    "En resumen, el parámetro `top_p` permite controlar el **equilibrio entre creatividad y precisión** en las respuestas generadas por el modelo, seleccionando solo aquellas palabras que sumen un determinado porcentaje de la probabilidad acumulada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1984\" de George Orwell es una obra clásica de la literatura distópica que presenta una visión sombría de un futuro totalitario y opresivo. Orwell crea una sociedad controlada por un gobierno autoritario que manipula la información, vigila a sus ciudadanos en todo momento y suprime cualquier forma de libertad individual.\n",
      "\n",
      "Esta novela tiene una relevancia atemporal ya que plantea cuestiones fundamentales sobre el poder, la manipulación y la vigilancia, temas que siguen siendo relevantes en la actualidad. La conexión entre \"1984\" y la agenda 2030 radica en la importancia de defender los derechos humanos, la libertad de expresión y la democracia para construir un mundo más justo y sostenible.\n",
      "\n",
      "La obra de Orwell nos invita a reflexionar sobre los peligros de un gobierno autoritario y sobre la necesidad de proteger nuestros derechos frente a posibles abusos de poder. En ese sentido, la agenda 2030 busca promover la paz, la justicia y la igualdad globalmente, fomentando la protección de los derechos humanos y la promoción de sociedades pacíficas, inclusivas y sostenibles para todos.\n",
      "\n",
      "En resumen, \"1984\" de George Orwell nos recuerda la importancia de defender nuestros valores democráticos y luchar contra la opresión y la manipulación, principios fundamentales que también están presentes en la agenda 2030 para el desarrollo sostenible. Es una lectura imprescindible que nos invita a reflexionar sobre el poder, la libertad y la resistencia en un mundo cada vez más complejo y desafiante.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {'role':'system','content':'Eres un buen asistente de ayuda'},\n",
    "        {'role':'user','content':'Escribe una reseña del libro 1984 de George Orwell y que relación tiene con la agenda 2030'},\n",
    "    ],\n",
    "    # temperature entre 0 y 2, default 1 the higher the allucination\n",
    "    top_p=1 #Si top_p es 0.1, el modelo solo considerará las palabras que en conjunto suman el 10% de la probabilidad acumulada.\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1984\" de George Orwell es una novela distópica que presenta un mundo totalitario en el que el gobierno controla todos los aspectos de la vida de sus ciudadanos, manipulando la información, reprimiendo la libertad de pensamiento y vigilando constantemente a la población a través de un sistema de vigilancia omnipresente.\n",
      "\n",
      "La novela aborda temas como la manipulación de la verdad, la pérdida de la privacidad y la supresión de la individualidad, mostrando cómo un régimen autoritario puede ejercer un control absoluto sobre la sociedad.\n",
      "\n",
      "En relación con la Agenda 2030, que es un plan de acción global para erradicar la pobreza, proteger el planeta y garantizar la prosperidad para todos, \"1984\" nos recuerda la importancia de defender los derechos humanos, la libertad de expresión y la democracia para evitar caer en sistemas opresivos y totalitarios.\n",
      "\n",
      "La novela nos invita a reflexionar sobre la importancia de proteger los valores democráticos y los derechos individuales, y a estar alerta ante cualquier intento de control excesivo por parte de los gobiernos o de cualquier entidad que amenace nuestra libertad y dignidad como seres humanos.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {'role':'system','content':'Eres un buen asistente de ayuda'},\n",
    "        {'role':'user','content':'Escribe una reseña del libro 1984 de George Orwell y que relación tiene con la agenda 2030'},\n",
    "    ],\n",
    "    # temperature entre 0 y 2, default 1 the higher the allucination\n",
    "    top_p=0.1 #Si top_p es 0.1, el modelo solo considerará las palabras que en conjunto suman el 10% de la probabilidad acumulada.\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro **`max_tokens`** en modelos de lenguaje como GPT-3.5 controla el **número máximo de tokens** que puede generar el modelo en una respuesta o completar una tarea.\n",
    "\n",
    "### ¿Qué es un \"token\"?\n",
    "\n",
    "Un **token** es una unidad de texto que puede representar desde una letra, un fragmento de una palabra, una palabra completa o incluso signos de puntuación. En el caso de modelos como GPT, el texto se divide en tokens antes de ser procesado. Por ejemplo:\n",
    "- La palabra \"futurista\" podría dividirse en dos tokens: \"futuris\" y \"ta\".\n",
    "- Palabras cortas como \"gato\" podrían ser un solo token.\n",
    "- Una frase completa, como \"Hola, ¿cómo estás?\", se puede dividir en varios tokens.\n",
    "\n",
    "### ¿Cómo funciona el parámetro `max_tokens`?\n",
    "\n",
    "- **`max_tokens`** determina la cantidad máxima de tokens que el modelo puede generar en su salida (respuesta) después de procesar el mensaje.\n",
    "  \n",
    "- Este límite incluye tanto las palabras de la respuesta como los signos de puntuación.\n",
    "\n",
    "### Ejemplo de uso\n",
    "\n",
    "Si configuras `max_tokens=50`, el modelo generará como máximo 50 tokens en su respuesta. Esto significa que el texto generado podría tener 50 tokens como máximo, aunque podría ser menos si la respuesta se completa antes.\n",
    "\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role':'system','content':'Eres un buen asistente de ayuda'},\n",
    "        {'role':'user','content':'Escribe una reseña breve de 1984 de George Orwell'}\n",
    "    ],\n",
    "    max_tokens=50 # La respuesta tendrá un máximo de 50 tokens.\n",
    ")\n",
    "```\n",
    "\n",
    "En este caso, el modelo limitará la longitud de la respuesta a no más de 50 tokens.\n",
    "\n",
    "### ¿Por qué es útil `max_tokens`?\n",
    "\n",
    "1. **Controlar la longitud de la respuesta**: Te permite definir cuán extensa será la salida. Si necesitas respuestas breves y concisas, puedes usar un valor bajo para `max_tokens`. Si quieres respuestas más detalladas, puedes usar un valor más alto.\n",
    "\n",
    "2. **Optimización de recursos**: En muchos servicios que usan modelos de lenguaje, el uso de tokens afecta los costos. Al limitar el número de tokens, puedes controlar cuánto procesamiento estás consumiendo y, por lo tanto, gestionar el costo.\n",
    "\n",
    "3. **Prevenir respuestas largas**: Si no quieres que el modelo genere respuestas demasiado largas o divagantes, puedes establecer un límite con `max_tokens` para mantener la respuesta directa y concisa.\n",
    "\n",
    "### Nota importante:\n",
    "- El número de tokens incluye tanto el texto de entrada (el mensaje o pregunta del usuario) como la respuesta generada por el modelo. Es importante tener esto en cuenta al definir el valor de `max_tokens`, ya que el modelo considera todos los tokens (entrada + salida).\n",
    "\n",
    "En resumen, **`max_tokens`** te permite controlar la longitud máxima de la respuesta generada por el modelo, lo que es útil para gestionar el tamaño de la salida y los recursos de procesamiento.\n",
    "\n",
    "## Tokens por defecto\n",
    "\n",
    "El parámetro **`max_tokens`** no tiene un valor por defecto explícito en muchos casos, y su límite depende del modelo específico que estés utilizando. \n",
    "\n",
    "Por ejemplo, para el modelo **`gpt-3.5-turbo`**, el límite máximo de tokens (incluyendo tanto los tokens de entrada como de salida) es de **4096 tokens**. Esto significa que si no especificas un valor para `max_tokens`, el modelo utilizará el número de tokens restante disponible después de procesar la entrada para generar la respuesta. \n",
    "\n",
    "Para otros modelos, este límite puede variar. Algunos ejemplos:\n",
    "\n",
    "- **`gpt-3.5-turbo`**: Límite total de 4096 tokens (entrada + salida).\n",
    "- **`gpt-4`**: Límite de hasta 8192 o 32,768 tokens, dependiendo de la versión.\n",
    "\n",
    "En resumen, **no hay un valor predeterminado** específico para `max_tokens`, pero el modelo usa la capacidad restante hasta su límite máximo si no se especifica un valor en particular. Si quieres limitar la longitud de la respuesta, necesitas establecer explícitamente un valor para `max_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1984\" de George Orwell es una obra maestra de la literatura distópica que sigue impactando a los lectores desde su publicación en 1949. La novela presenta un sombrío mundo totalitario\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role':'system','content':'Eres un buen asistente de ayuda'},\n",
    "        {'role':'user','content':'Escribe una reseña de 1984 de George Orwell'}\n",
    "    ],\n",
    "    max_tokens=50 # La respuesta tendrá un máximo de 50 tokens.\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1984\" de George Orwell es una novela distópica que presenta un sombrío futuro totalitario en el que el gobierno controla todos los aspectos de la vida de sus ciudadanos. La historia sigue a Winston Smith, un hombre que trabaja en el Ministerio de la Verdad y comienza a cuestionar la realidad en la que vive. A medida que se adentra en la rebelión contra el régimen autoritario, se ve envuelto en una trama de vigilancia, manipulación y represión.\n",
      "\n",
      "Orwell crea un mundo opresivo y aterrador donde la libertad individual y la verdad son sacrificadas en aras del poder y el control. A través de su prosa incisiva y su narrativa intensa, el autor nos sumerge en la lucha de Winston por encontrar la verdad y la esperanza en un mundo dominado por la manipulación y la opresión.\n",
      "\n",
      "\"1984\" es una obra maestra de la literatura distópica que sigue siendo relevante en la actualidad por su exploración de temas como la vigilancia, la censura, la manipulación de la información y la resistencia frente a la opresión. Es una lectura provocativa y perturbadora que desafía al lector a reflexionar sobre el poder, la libertad y el control en la sociedad.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role':'system','content':'Eres un buen asistente de ayuda'},\n",
    "        {'role':'user','content':'Escribe una reseña de 1984 de George Orwell'}\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro **`stop`** en los modelos de lenguaje como GPT-3.5 o GPT-4 se utiliza para definir una o más secuencias de texto que, cuando el modelo las encuentra al generar una respuesta, provocan que se detenga la generación. Este parámetro permite un mayor control sobre dónde y cómo termina la salida del modelo.\n",
    "\n",
    "### ¿Cómo funciona el parámetro `stop`?\n",
    "\n",
    "Cuando defines el parámetro `stop`, le indicas al modelo que deje de generar más tokens si encuentra una de las secuencias que has definido. Estas secuencias pueden ser palabras, frases o incluso signos de puntuación.\n",
    "\n",
    "### Ejemplo:\n",
    "\n",
    "Si defines un valor para `stop`, como `stop=[\"\\n\"]`, el modelo dejará de generar texto en cuanto encuentre un salto de línea (`\\n`). Veamos un ejemplo práctico:\n",
    "\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role':'system', 'content':'Eres un buen asistente de ayuda'},\n",
    "        {'role':'user', 'content':'Escribe una reseña del libro 1984 de George Orwell'}\n",
    "    ],\n",
    "    stop=[\".\"]  # El modelo se detendrá al encontrar un punto.\n",
    ")\n",
    "```\n",
    "\n",
    "En este ejemplo, el modelo dejará de generar más texto en cuanto llegue a un punto (`.`), lo que puede ser útil si quieres limitar la respuesta a una sola oración.\n",
    "\n",
    "### Usos comunes del parámetro `stop`:\n",
    "\n",
    "1. **Controlar la longitud de la respuesta**: Si esperas que el modelo genere varias oraciones, pero quieres detenerlo en cierto punto (por ejemplo, al finalizar una oración o un párrafo), puedes usar el parámetro `stop` para definir dónde debería parar.\n",
    "   \n",
    "2. **Crear diálogos estructurados**: Si estás generando un diálogo entre personajes y deseas que el modelo detenga su generación cuando cambie de turno de habla, puedes establecer secuencias de parada para facilitar esto.\n",
    "\n",
    "3. **Detener el texto en secciones específicas**: Puedes detener la salida del modelo cuando genere una cierta palabra o frase que defina el final de una sección.\n",
    "\n",
    "### Ejemplo con múltiples secuencias de parada:\n",
    "\n",
    "También puedes proporcionar varias secuencias de parada, y el modelo se detendrá cuando encuentre cualquiera de ellas. Aquí tienes otro ejemplo:\n",
    "\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role':'system', 'content':'Eres un buen asistente de ayuda'},\n",
    "        {'role':'user', 'content':'Describe el clima de hoy y luego sugiere un plan para el día'}\n",
    "    ],\n",
    "    stop=[\"clima\", \"día\"]  # El modelo se detendrá si encuentra \"clima\" o \"día\".\n",
    ")\n",
    "```\n",
    "\n",
    "### Características clave del parámetro `stop`:\n",
    "\n",
    "- Puedes definir una o más secuencias de parada (en forma de lista).\n",
    "- Cada vez que el modelo encuentra alguna de esas secuencias, deja de generar texto adicional.\n",
    "- Las secuencias de parada pueden ser cualquier fragmento de texto que desees (palabras, signos de puntuación, etc.).\n",
    "- Es útil cuando necesitas controlar el flujo o la estructura de la salida generada.\n",
    "\n",
    "### Resumen:\n",
    "\n",
    "El parámetro **`stop`** es una herramienta para controlar cuándo y cómo el modelo debe detenerse al generar texto. Le dices al modelo que deje de generar más tokens cuando se encuentre con alguna de las secuencias de parada definidas. Esto puede ayudarte a estructurar mejor la salida, controlar su longitud o ajustar el contenido generado de acuerdo con tus necesidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1984\" de George Orwell es una obra maestra distópica que ha dejado una marca indeleble en la literatura mundial\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role':'system', 'content':'Eres un buen asistente de ayuda'},\n",
    "        {'role':'user', 'content':'Escribe una reseña del libro 1984 de George Orwell'}\n",
    "    ],\n",
    "    stop=[\".\"]  # El modelo se detendrá al encontrar un punto.\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Una noche soleada es un fenómeno particular en el que el sol aún brilla en el cielo a pesar de que es de noche. En este caso, el \n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role':'system', 'content':'Eres un buen asistente de ayuda'},\n",
    "        {'role':'user', 'content':'Describe el clima de una noche soleada'}\n",
    "    ],\n",
    "    stop=[\"clima\", \"día\"]  # El modelo se detendrá si encuentra \"clima\" o \"día\".\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro **`presence_penalty`** en los modelos de lenguaje como GPT-3.5 o GPT-4 ajusta la forma en que el modelo genera texto, **penalizando la repetición de temas o palabras ya mencionadas**. En esencia, este parámetro promueve la creatividad y la generación de contenido novedoso al hacer que el modelo sea menos propenso a repetir información que ya ha proporcionado.\n",
    "\n",
    "### ¿Cómo funciona `presence_penalty`?\n",
    "\n",
    "El **`presence_penalty`** influye en el comportamiento del modelo al aplicar una penalización a las palabras que ya han aparecido en la conversación o en el texto generado. Esto significa que el modelo será menos propenso a repetir esas palabras y tratará de utilizar palabras nuevas o diferentes.\n",
    "\n",
    "- Si el valor de **`presence_penalty`** es **alto**, el modelo tendrá más aversión a repetir palabras o ideas ya mencionadas, lo que fomentará la introducción de términos y temas nuevos.\n",
    "- Si el valor es **bajo o cero**, el modelo no tendrá problemas en repetir palabras o conceptos que ya han sido utilizados anteriormente en la conversación.\n",
    "\n",
    "### Ejemplo:\n",
    "\n",
    "Supongamos que estás generando una descripción de un producto y quieres que el modelo sea más diverso en el lenguaje y que evite repetir las mismas palabras.\n",
    "\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role':'system', 'content':'Eres un asistente de marketing.'},\n",
    "        {'role':'user', 'content':'Describe este producto: un teléfono móvil con cámara de alta calidad y pantalla de 6.5 pulgadas.'}\n",
    "    ],\n",
    "    presence_penalty=1.5  # Penaliza las repeticiones para promover más variedad en el lenguaje.\n",
    ")\n",
    "```\n",
    "\n",
    "En este caso, un valor alto como `1.5` hará que el modelo evite repetir palabras como \"cámara\" o \"pantalla\" en la descripción, y buscará sinónimos o expresiones alternativas. Esto puede llevar a una descripción más diversa y creativa.\n",
    "\n",
    "### Rango de valores:\n",
    "\n",
    "- Los valores de **`presence_penalty`** suelen estar en un rango entre **-2.0 y 2.0**:\n",
    "  - **Un valor negativo** fomentaría la repetición de palabras o conceptos.\n",
    "  - **Un valor positivo** desalentará las repeticiones, promoviendo el uso de palabras o frases nuevas.\n",
    "  \n",
    "Por ejemplo:\n",
    "- **`presence_penalty=0`**: No hay penalización, el modelo puede repetir palabras o ideas sin problema.\n",
    "- **`presence_penalty=2.0`**: Penalización fuerte, el modelo intentará evitar repetirse lo máximo posible, introduciendo más diversidad en el texto generado.\n",
    "\n",
    "### ¿Cuándo usar `presence_penalty`?\n",
    "\n",
    "- **Diversidad lingüística**: Si deseas que el texto generado sea más variado y evite la repetición de palabras o conceptos.\n",
    "- **Evitar monotonía**: Si el texto tiende a ser redundante y prefieres que el modelo utilice una gama más amplia de vocabulario.\n",
    "- **Creatividad**: Si quieres que el modelo introduzca ideas nuevas y no se estanque en conceptos ya discutidos.\n",
    "\n",
    "### Resumen:\n",
    "\n",
    "El parámetro **`presence_penalty`** controla la tendencia del modelo a repetir palabras o temas ya mencionados. Un valor alto de `presence_penalty` desalienta la repetición y fomenta la creatividad, mientras que un valor bajo o negativo permite más repeticiones en el texto generado. Es útil cuando se busca diversidad y novedad en el contenido que genera el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Claro! Este teléfono móvil combina lo mejor en tecnología de cámaras y pantallas. Con su cámara de alta calidad, podrás capturar imágenes y videos con una nitidez excepcional. Además, su pantalla de 6.5 pulgadas te permitirá disfrutar de una experiencia visual inmersiva para ver tus fotos, videos y aplicaciones favoritas con colores vibrantes y detalles nítidos. ¡Un dispositivo perfecto para los amantes de la fotografía y el entretenimiento multimedia!\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role':'system', 'content':'Eres un asistente de marketing.'},\n",
    "        {'role':'user', 'content':'Describe este producto: un teléfono móvil con cámara de alta calidad y pantalla de 6.5 pulgadas.'}\n",
    "    ],\n",
    "    presence_penalty=1.5  # Penaliza las repeticiones para promover más variedad en el lenguaje.\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez un gato callejero llamado Ginger, con un pelaje naranja brillante y unos ojos verdes brillantes. Ginger vivía en un callejón en el centro de la ciudad, donde pasaba sus días explorando y buscando comida.\n",
      "\n",
      "Un día, Ginger se topó con un grupo de gatos callejeros que lo invitaron a unirse a su pandilla. Intrigado por la idea de tener compañía y protección, Ginger aceptó y se unió a ellos en sus travesuras por la ciudad.\n",
      "\n",
      "Juntos, Ginger y los otros gatos exploraban los callejones, cazaban ratones y se escondían de los perros callejeros. Ginger se convirtió en un miembro valioso de la pandilla, con su agilidad y astucia siendo de gran ayuda en sus aventuras.\n",
      "\n",
      "Un día, mientras exploraban un parque abandonado, Ginger y sus amigos se encontraron con un grupo de gatos callejeros rivales. Se desató una pelea feroz, con maullidos y arañazos por todas partes. Ginger luchó valientemente para proteger a sus amigos, demostrando su valía una vez más.\n",
      "\n",
      "Al final, la pandilla de Ginger logró ahuyentar a los gatos rivales y celebraron su victoria con comida encontrada en los contenedores de basura. Ginger se sentía feliz de tener amigos leales a su lado y se dio cuenta de que, a pesar de su vida en la calle, había encontrado un lugar al que pertenecer.\n",
      "\n",
      "Desde ese día en adelante, Ginger y su pandilla continuaron sus aventuras juntos, enfrentando desafíos y viviendo sus días con valentía y camaradería en las peligrosas calles de la ciudad. Y a pesar de los desafíos que enfrentaban, Ginger sabía que siempre tendría a sus amigos a su lado mientras exploraban el mundo juntos.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role':'system', 'content':'Eres un narrador de historias.'},\n",
    "        {'role':'user', 'content':'Escribe una historia corta sobre un gato llamado Ginger y sus aventuras.'}\n",
    "    ],\n",
    "    presence_penalty=-2  # Evitará repetir constantemente \"Ginger\" y buscará más variedad en el lenguaje.\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
